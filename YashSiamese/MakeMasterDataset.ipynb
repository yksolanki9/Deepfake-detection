{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92f0130d5c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msavez_compressed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "# face detection for the 5 Celebrity Faces Dataset\n",
    "# !pip install Pillow\n",
    "# !pip install matplotlib\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isdir\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "from numpy import savez_compressed\n",
    "from numpy import asarray\n",
    "import time\n",
    "\n",
    "abs_path = \"/raid/Data/Master_Dataset_Elvin_Mrunal/Facebook_created_dataset_Face_extracted_Real+Fake/train/\"\n",
    "new_path = \"/scratch/ysolanki/SiameseFacebook/MasterDataset/\"\n",
    "\n",
    "if not os.path.exists(new_path):\n",
    "    os.makedirs(new_path)\n",
    "\n",
    "def extract_face(filename):\n",
    "    image = Image.open(filename)\n",
    "    image = image.resize((160, 160))\n",
    "    face_array = asarray(image)\n",
    "    return face_array\n",
    "    \n",
    "# load images and extract faces for all images in a directory\n",
    "def load_faces(directory):\n",
    "    faces = list()\n",
    "    # enumerate file\n",
    "    count = 0\n",
    "    start = time.time()\n",
    "    for filename in listdir(directory):\n",
    "        # path\n",
    "        path = directory + filename\n",
    "        # get face\n",
    "        face = extract_face(path)\n",
    "#         if not face:\n",
    "#             continue\n",
    "        # store\n",
    "        faces.append(face)\n",
    "        \n",
    "        count += 1\n",
    "        if count %1000 == 0:\n",
    "            end = time.time()\n",
    "            print(face.shape)\n",
    "            print(\"No. of images done: \", count, \" and time taken is \", (end-start)/60)\n",
    "            start = time.time()\n",
    "    return faces\n",
    "\n",
    "# load a dataset that contains one subdir for each class that in turn contains images\n",
    "def load_dataset(directory):\n",
    "    X, y = list(), list()\n",
    "    # enumerate folders, on per class\n",
    "    for subdir in listdir(directory):\n",
    "        # path\n",
    "        path = directory + subdir + '/'\n",
    "        # skip any files that might be in the dir\n",
    "        if not isdir(path):\n",
    "            continue\n",
    "        print(subdir)\n",
    "        # load all faces in the subdirectory\n",
    "        faces = load_faces(path)\n",
    "        # create labels\n",
    "        labels = [subdir for _ in range(len(faces))]\n",
    "        # summarize progress\n",
    "        print('>loaded %d examples for class: %s' % (len(faces), subdir))\n",
    "        # store\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return asarray(X), asarray(y)\n",
    " \n",
    "# load train dataset\n",
    "trainX, trainy = load_dataset(abs_path)\n",
    "print(trainX.shape, trainy.shape)\n",
    "# load test dataset\n",
    "# testX, testy = load_dataset('5-celebrity-faces-dataset/val/')\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed(new_path + 'Master-dataset-from-elvin-images-160x160.npz', trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[90, 66, 64],\n",
       "        [90, 66, 64],\n",
       "        [89, 65, 61],\n",
       "        ...,\n",
       "        [41, 30, 26],\n",
       "        [41, 30, 26],\n",
       "        [41, 30, 26]],\n",
       "\n",
       "       [[90, 66, 64],\n",
       "        [89, 65, 63],\n",
       "        [89, 65, 61],\n",
       "        ...,\n",
       "        [41, 30, 26],\n",
       "        [41, 30, 26],\n",
       "        [41, 30, 26]],\n",
       "\n",
       "       [[89, 65, 63],\n",
       "        [89, 65, 63],\n",
       "        [88, 64, 60],\n",
       "        ...,\n",
       "        [41, 30, 26],\n",
       "        [41, 30, 26],\n",
       "        [41, 30, 26]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[46, 30, 30],\n",
       "        [44, 28, 29],\n",
       "        [42, 26, 27],\n",
       "        ...,\n",
       "        [73, 58, 55],\n",
       "        [76, 61, 58],\n",
       "        [77, 62, 59]],\n",
       "\n",
       "       [[46, 30, 30],\n",
       "        [44, 28, 29],\n",
       "        [41, 25, 26],\n",
       "        ...,\n",
       "        [73, 58, 55],\n",
       "        [75, 60, 57],\n",
       "        [77, 62, 59]],\n",
       "\n",
       "       [[45, 29, 29],\n",
       "        [43, 27, 28],\n",
       "        [41, 25, 26],\n",
       "        ...,\n",
       "        [72, 57, 54],\n",
       "        [75, 60, 57],\n",
       "        [76, 61, 58]]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"Master-dataset-from-elvin-images-160x160.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(new_path):\n",
    "    os.makedirs(new_path)\n",
    "savez_compressed(new_path + 'Master-dataset-from-elvin-images.npz', trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/raid/Data/Master_Dataset_Elvin_Mrunal/Facebook_created_dataset_Face_extracted_Real+Fake/train\n"
     ]
    }
   ],
   "source": [
    "cd /raid/Data/Master_Dataset_Elvin_Mrunal/Facebook_created_dataset_Face_extracted_Real+Fake/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mfake\u001b[0m/  \u001b[01;34mreal\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-840cb3f7cbb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msavez_compressed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# calculate a face embedding for each face in the dataset using facenet\n",
    "# !pip install keras\n",
    "from numpy import load\n",
    "from numpy import expand_dims\n",
    "from numpy import asarray\n",
    "from numpy import savez_compressed\n",
    "from keras.models import load_model\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "new_path = \"/scratch/ysolanki/SiameseFacebook/MasterDataset/\"\n",
    "\n",
    "\n",
    "# # get the face embedding for one face\n",
    "def get_embedding(model, face_pixels):\n",
    "    # scale pixel values\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    # standardize pixel values across channels (global)\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    # transform face into one sample\n",
    "    print(face_pixels.shape)\n",
    "    samples = expand_dims(face_pixels, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]\n",
    "\n",
    "\n",
    "# load the face dataset\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "data = load(new_path + 'Master-dataset-from-elvin-images-160x160.npz')\n",
    "np.load.__defaults__=(None, False, True, 'ASCII')\n",
    "print(data)\n",
    "trainX, trainy = data['arr_0'], data['arr_1']\n",
    "print('Loaded: ', trainX.shape, trainy.shape)\n",
    "# load the facenet model\n",
    "model = load_model('facenet_keras.h5')\n",
    "print('Loaded Model')\n",
    "# convert each face in the train set to an embedding\n",
    "\n",
    "# embedder = FaceNet()\n",
    "\n",
    "newTrainX = list()\n",
    "count = 0\n",
    "start = time.time()\n",
    "for face_pixels in trainX:\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "#     pixels = expand_dims(face_pixels, axis=0)\n",
    "#     print(pixels)\n",
    "#     print(pixels.shape)\n",
    "#     embedding = embedder.embeddings(pixels)\n",
    "    newTrainX.append(embedding)\n",
    "    count += 1\n",
    "    if count%1000 == 0:\n",
    "        end = time.time()\n",
    "        print(\"No. of images done: \", count, \" and time taken is \", (end-start)/60)\n",
    "        start = time.time()\n",
    "newTrainX = asarray(newTrainX)\n",
    "print(newTrainX.shape)\n",
    "# convert each face in the test set to an embedding\n",
    "# newTestX = list()\n",
    "# for face_pixels in testX:\n",
    "#     embedding = get_embedding(model, face_pixels)\n",
    "#     newTestX.append(embedding)\n",
    "# newTestX = asarray(newTestX)\n",
    "# print(newTestX.shape)\n",
    "# save arrays to one file in compressed format\n",
    "savez_compressed(new_path + 'Master-dataset-elvin-faces-embeddings-160x160.npz', newTrainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(496186, 128)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "new_path = \"/scratch/ysolanki/SiameseFacebook/MasterDataset/\"\n",
    "file1 = \"Master-dataset-elvin-faces-embeddings-160x160.npz\"\n",
    "file2 = \"Master-dataset-from-elvin-images-160x160.npz\"\n",
    "\n",
    "data = np.load(new_path + file1)\n",
    "print(data['arr_0'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ValueError: Object arrays cannot be loaded when allow_pickle=False\n",
    "\n",
    "# import numpy as np\n",
    "# # save np.load\n",
    "# np_load_old = np.load\n",
    "\n",
    "# # modify the default parameters of np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# # call load_data with allow_pickle implicitly set to true\n",
    "# data = np.load(new_path + 'Master-dataset-from-elvin-images.npz')\n",
    "\n",
    "# # restore np.load for future normal usage\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
